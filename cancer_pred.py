# -*- coding: utf-8 -*-
"""Copy of AMLBS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lJ4L0iviYzVoDF8S4AFUbBoVfl-IxpDY

# EDA
Data Summary and Description: Start by providing a summary of the dataset, including basic statistics like mean, median, standard deviation, and percentiles for numerical features. For categorical features, present the frequency distribution of categories. Describe the size of the dataset (number of rows and columns) and the data types of each column.

Data Visualization: Create a variety of data visualizations to explore the dataset visually. These may include histograms, box plots, scatter plots, bar charts, and more. Visualization can reveal patterns, outliers, and distributions within the data.

Data Cleaning: Identify and address missing values, outliers, and any inconsistencies in the data. Describe your approach to handling missing data and outliers.

Feature Exploration: Examine individual features (variables) to understand their distributions, relationships with the target variable (if applicable), and any patterns or trends. This can involve visualizations and summary statistics for each feature.

Correlation Analysis: Explore correlations between numerical features using correlation matrices or scatter plots. Identify which features are strongly correlated and which are not.

Multivariate Analysis: Investigate relationships between multiple variables simultaneously. This can include techniques like heatmaps, pair plots, and dimensionality reduction methods (e.g., PCA).

Hypothesis Testing: If applicable, conduct hypothesis tests to validate assumptions or explore relationships. For example, t-tests, chi-squared tests, or ANOVA tests can be used depending on the nature of your data and research questions.

Data Insights: Summarize your findings and insights from the EDA process. This can include hypotheses about the data, patterns you've observed, and potential areas for further investigation.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset from a CSV file
file_path = '/content/drive/MyDrive/MLBS/framingham.csv'
df = pd.read_csv(file_path)

# Data Summary and Description
def summarize_data(df):
    print("Dataset Summary:")
    print(df.info())

    print("\nDescriptive Statistics for Numerical Features:")
    print(df.describe())

    categorical_features = df.select_dtypes(include=['object']).columns
    for feature in categorical_features:
        print(f"\nFrequency Distribution for {feature}:")
        print(df[feature].value_counts())

def visualize_data(df):
    print("\nData Visualization:")
    df.hist(bins=20, figsize=(15, 10))
    plt.show()

def clean_data(df):
    print("\nData Cleaning:")
    df.fillna(df.mean(), inplace=True)

def explore_features(df):
    print("\nFeature Exploration:")
    print("\nBox Plots for Numerical Features:")
    for feature in df.columns:
        if df[feature].dtype != 'object':
            plt.figure(figsize=(6, 4))
            df.boxplot(column=feature)
            plt.title(f"Box Plot of {feature}")
            plt.show()

def analyze_correlation(df):
    print("\nCorrelation Analysis:")
    correlation_matrix = df.corr()
    print("Correlation Matrix:")
    print(correlation_matrix)

def visualize_class_distribution(df):
    print("\nVisualizing class distribution (assuming binary classification):")
    plt.figure(figsize=(6, 4))
    df['TenYearCHD'].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])
    plt.title("Class Distribution")
    plt.xlabel("Label")
    plt.ylabel("Count")
    plt.xticks(rotation=0)
    plt.show()

# Perform data summarization
summarize_data(df)

# Visualize the data
visualize_data(df)

# Clean the data
clean_data(df)

# Explore features
explore_features(df)

# Analyze correlations
analyze_correlation(df)

# Visualize class distribution
visualize_class_distribution(df)

"""# Dealing with the NA (missing values) and applying PCA for different number of features

PCA applied for 5, 8, 12, 14 features
"""

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset from a CSV file
file_path = '/content/drive/MyDrive/MLBS/framingham.csv'
df = pd.read_csv(file_path)

# Data Summary and Description
def summarize_data(df):
    """
    Summarize the dataset by displaying information, descriptive statistics,
    and frequency distributions for categorical features.

    Args:
        df (pd.DataFrame): The dataset to be summarized.
    """
    print("Dataset Summary:")
    print(df.info())

    print("\nDescriptive Statistics for Numerical Features:")
    print(df.describe())

    categorical_features = df.select_dtypes(include=['object']).columns
    for feature in categorical_features:
        print(f"\nFrequency Distribution for {feature}:")
        print(df[feature].value_counts())

def visualize_data(df):
    """
    Visualize the dataset using histograms for numerical features.

    Args:
        df (pd.DataFrame): The dataset to be visualized.
    """
    print("\nData Visualization:")
    df.hist(bins=20, figsize=(15, 10))
    plt.show()

def clean_data(df):
    """
    Clean the dataset by filling missing values with the mean for numerical features.

    Args:
        df (pd.DataFrame): The dataset to be cleaned.
    """
    print("\nData Cleaning:")
    df.fillna(df.mean(), inplace=True)

def explore_features(df):
    """
    Explore dataset features by creating box plots for numerical features.

    Args:
        df (pd.DataFrame): The dataset to explore.
    """
    print("\nFeature Exploration:")
    print("\nBox Plots for Numerical Features:")
    for feature in df.columns:
        if df[feature].dtype != 'object':
            plt.figure(figsize=(6, 4))
            df.boxplot(column=feature)
            plt.title(f"Box Plot of {feature}")
            plt.show()

def analyze_correlation(df):
    """
    Analyze feature correlations by displaying a correlation matrix.

    Args:
        df (pd.DataFrame): The dataset to analyze.
    """
    print("\nCorrelation Analysis:")
    correlation_matrix = df.corr()
    print("Correlation Matrix:")
    print(correlation_matrix)

def visualize_class_distribution(df):
    """
    Visualize the class distribution for binary classification.

    Args:
        df (pd.DataFrame): The dataset with class labels.
    """
    print("\nVisualizing class distribution (assuming binary classification):")
    plt.figure(figsize=(6, 4))
    df['TenYearCHD'].value_counts

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Load your dataset from a CSV file
file_path = '/content/drive/MyDrive/MLBS/framingham.csv'
df = pd.read_csv(file_path)

# Handle missing values: Replace NAs with mode for binary features and mean for non-binary features
for column in df.columns:
    if df[column].dtypes == 'int64' or df[column].dtypes == 'float64':
        if df[column].nunique() <= 2:  # Binary feature
            mode_value = df[column].mode().iloc[0]
            df[column].fillna(mode_value, inplace=True)
        else:  # Non-binary feature
            mean_value = df[column].mean()
            df[column].fillna(mean_value, inplace=True)

# Separate features (X) and labels (y)
X = df.drop(columns=['TenYearCHD'])
y = df['TenYearCHD']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply PCA for feature scaling
n_components = 14  # You can change this value to the desired number of components
pca = PCA(n_components=n_components)

# Fit PCA to the training data and transform both training and testing data
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Create and train your machine learning models
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier()
}

results = {}

# Loop through each machine learning model
for model_name, model in models.items():
    # Train the model on the PCA-transformed training data
    model.fit(X_train_pca, y_train)

    # Make predictions on the PCA-transformed testing data
    y_pred = model.predict(X_test_pca)

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    # Store the results in a dictionary
    results[model_name] = {
        "Accuracy": accuracy,
        "Precision": precision,
        "F1 Score": f1,
        "Recall": recall
    }

# Display the results for each model
for model_name, metrics in results.items():
    print(f"{model_name}:")
    print(f"Accuracy: {metrics['Accuracy']:.2f}")
    print(f"Precision: {metrics['Precision']:.2f}")
    print(f"Recall: {metrics['Recall']:.2f}")
    print(f"F1 Score: {metrics['F1 Score']:.2f}")
    print()

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

# Load the dataset from a CSV file
file_path = '/content/drive/MyDrive/MLBS/framingham.csv'
df = pd.read_csv(file_path)

# Separate features (X) and labels (y)
X = df.drop(columns=['TenYearCHD'])
y = df['TenYearCHD']

# Identify binary columns (containing only 0 and 1)
binary_columns = []
for col in X.columns:
    unique_values = X[col].unique()
    if len(unique_values) == 2 and 0 in unique_values and 1 in unique_values:
        binary_columns.append(col)

# Create separate dataframes for binary and non-binary columns
X_binary = X[binary_columns]
X_non_binary = X.drop(columns=binary_columns)

# Handle missing values for non-binary columns using mean
imputer_mean = SimpleImputer(strategy='mean')
X_non_binary_imputed = pd.DataFrame(imputer_mean.fit_transform(X_non_binary), columns=X_non_binary.columns)

# Handle missing values for binary columns using mode
imputer_mode = SimpleImputer(strategy='most_frequent')
X_binary_imputed = pd.DataFrame(imputer_mode.fit_transform(X_binary), columns=X_binary.columns)

# Concatenate the imputed dataframes back together
X_imputed = pd.concat([X_non_binary_imputed, X_binary_imputed], axis=1)

# Perform PCA for various numbers of components (5, 8, 12, 14)
num_components = [5, 8, 12, 14]

for n in num_components:
    # Create PCA model with the specified number of components
    pca = PCA(n_components=n)

    # Fit and transform the data
    X_pca = pca.fit_transform(X_imputed)

    # Explained variance ratio
    explained_variance = pca.explained_variance_ratio_

    # Cumulative explained variance
    cumulative_explained_variance = explained_variance.cumsum()

    # Plot explained variance and cumulative explained variance
    plt.figure(figsize=(8, 4))
    plt.bar(range(1, n + 1), explained_variance, alpha=0.5, align='center', label='Explained Variance')
    plt.step(range(1, n + 1), cumulative_explained_variance, where='mid', label='Cumulative Explained Variance')
    plt.xlabel('Principal Component')
    plt.ylabel('Explained Variance Ratio')
    plt.title(f'PCA with {n} Components (After Handling Missing Values)')
    plt.legend()
    plt.show()

    # Print the explained variance for the selected components
    print(f'Explained Variance for {n} Components: {explained_variance.sum():.2f}')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import classification_report

# Load the dataset from a CSV file
file_path = '/content/drive/MyDrive/MLBS/framingham.csv'
df = pd.read_csv(file_path)

# Separate features (X) and labels (y)
X = df.drop(columns=['TenYearCHD'])
y = df['TenYearCHD']

# Identify binary columns (containing only 0 and 1)
binary_columns = []
for col in X.columns:
    unique_values = X[col].unique()
    if len(unique_values) == 2 and 0 in unique_values and 1 in unique_values:
        binary_columns.append(col)

# Create separate dataframes for binary and non-binary columns
X_binary = X[binary_columns]
X_non_binary = X.drop(columns=binary_columns)

# Handle missing values for non-binary columns using mean
imputer_mean = SimpleImputer(strategy='mean')
X_non_binary_imputed = pd.DataFrame(imputer_mean.fit_transform(X_non_binary), columns=X_non_binary.columns)

# Handle missing values for binary columns using mode
imputer_mode = SimpleImputer(strategy='most_frequent')
X_binary_imputed = pd.DataFrame(imputer_mode.fit_transform(X_binary), columns=X_binary.columns)

# Concatenate the imputed dataframes back together
X_imputed = pd.concat([X_non_binary_imputed, X_binary_imputed], axis=1)

# Resample the data using Random Oversampling
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_imputed, y)

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Standardize features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train and evaluate models
models = {
    'SVM': SVC(),
    'Logistic Regression': LogisticRegression(),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier()
}

for model_name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    report = classification_report(y_test, y_pred)
    print(f'{model_name} Classification Report:\n{report}')